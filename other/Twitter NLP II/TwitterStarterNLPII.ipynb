{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import these two corpora. We will use them to clean our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Twitter data on 'entrepreneurship'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_train.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text in the tweets to all lower case. Remove the stopwords (such as 'a', 'the', and 'to') from the tweets. Keep each word's lemma, e.g., the verbs gone, going, and went have the lemma go. The lemmatizer needs to know the word's part of speech (pos). Use the regular expressions package re to remove http, RT, numbers and punctuations (except # and @), and whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are three examples of lemmatized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer.lemmatize('entrepreneurs', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer.lemmatize('bought', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer.lemmatize('starting', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopset = list(set(stopwords.words('english')))\n",
    "clean_tweets_text = []\n",
    "for tweet in df['Text']:  # Loop through the tokens (the words or symbols) in each tweet.   \n",
    "    cleaned_tweet = re.sub(r\"(RT)\",\" \", tweet)  # Remove RT.\n",
    "    cleaned_tweet = cleaned_tweet.lower()  # Convert the text to lower case\n",
    "    cleaned_tweet = ' '.join([word for word in cleaned_tweet.split() if word not in stopset])  # Keep only words that are not stopwords.\n",
    "    cleaned_tweet = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='n') for word in cleaned_tweet.split()])  # Keep each noun's lemma.\n",
    "    cleaned_tweet = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='v') for word in cleaned_tweet.split()])  # Keep each verb's lemma.\n",
    "    cleaned_tweet = re.sub(r\"amp\",\" \", cleaned_tweet)  # Remove the word \"amp\".\n",
    "    cleaned_tweet = re.sub(r\"(http\\S+)\",\" \", cleaned_tweet)  # Remove http links.\n",
    "    cleaned_tweet = re.sub(\"[^a-zA-Z#@]\",\" \", cleaned_tweet)  # Remove numbers and punctuations except # and @.\n",
    "    cleaned_tweet = ' '.join(cleaned_tweet.split())  # Remove white space.\n",
    "    cleaned_tweet = cleaned_tweet.replace('entrepreneurship', '')  # Replace your key words.\n",
    "    cleaned_tweet = cleaned_tweet.replace('entrepreneur', '')  # Replace your key words.\n",
    "    cleaned_tweet = cleaned_tweet.replace('entrepreneurial', '')  # Replace your key words.\n",
    "    clean_tweets_text.append(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanTweetText'] = clean_tweets_text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the counts for each of retweet count levels. Are most tweets retweeted or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[df['RT_Count_in_TimeWindow'] < 10]\n",
    "new_df['RT_Count_in_TimeWindow'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two subsetted data frames -- one for tweets without any retweets and another for tweets that were retweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_no_retweet = df[df['RT_Count_in_TimeWindow'] == 0]\n",
    "df_some_retweet = df[df['RT_Count_in_TimeWindow'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_some_retweet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_long_string =  ' '.join(df['cleanTweetText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(one_long_string)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('All tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_long_string_no_retweet =  ' '.join(df_no_retweet['cleanTweetText'])\n",
    "one_long_string_some_retweet =  ' '.join(df_some_retweet['cleanTweetText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(one_long_string_no_retweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(one_long_string_some_retweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_long_string_no_retweet = one_long_string_no_retweet.replace('amp', '')\n",
    "one_long_string_some_retweet = one_long_string_some_retweet.replace('amp', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_no_retweet = WordCloud(random_state=201).generate(one_long_string_no_retweet)\n",
    "wordcloud_some_retweet = WordCloud(random_state=201).generate(one_long_string_some_retweet)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(121)  # 121 means 1 row and 2 columns of plots and this is the first subplot\n",
    "plt.imshow(wordcloud_no_retweet)\n",
    "plt.axis('off')\n",
    "plt.title('Non-retweeted tweets')\n",
    "\n",
    "plt.subplot(122)  # 122 means 1 row and 2 columns of plots and this is the second subplot\n",
    "plt.imshow(wordcloud_some_retweet)\n",
    "plt.axis('off')\n",
    "plt.title('Retweeted tweets')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corpus (a list of all your documents). Find all the one-word phrases (unigrams) and two-word phrases (bigrams). We could keep going higher, to find all n-word phrases (ngrams). Below we create a \"bag of words\" for the top 20 phrases (unigrams or bigrams in this case). A bag of words (or document-term matrix) is a data frame of phrase counts. Each row is a document (or tweet in this case). The columns correspond to a phrase in any of the documents. An entry in the data frame is a count of the times the phrase appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(df['cleanTweetText'])\n",
    "corpus_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=30)\n",
    "bag_of_words = corpus_vectorizer.fit_transform(corpus)\n",
    "bag_of_words_df = pd.DataFrame(bag_of_words.toarray(), columns=corpus_vectorizer.get_feature_names())\n",
    "bag_of_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bag_of_words_df.mean(axis=0), index=bag_of_words_df.columns, columns=['Avg count (all tweets)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two bag of words -- one for tweets with no retweets and another for tweets with some retweets. Are the frequently used words different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_no_retweet = list(df_no_retweet['cleanTweetText'])\n",
    "corpus_no_retweet_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=20)\n",
    "bag_of_words_no_retweet = corpus_no_retweet_vectorizer.fit_transform(corpus_no_retweet)\n",
    "bag_of_words_no_retweet_df = pd.DataFrame(bag_of_words_no_retweet.toarray(), columns=corpus_no_retweet_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_some_retweet = list(df_some_retweet['cleanTweetText'])\n",
    "corpus_some_retweet_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=20)\n",
    "bag_of_words_some_retweet = corpus_some_retweet_vectorizer.fit_transform(corpus_some_retweet)\n",
    "bag_of_words_some_retweet_df = pd.DataFrame(bag_of_words_some_retweet.toarray(), columns=corpus_some_retweet_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_words_no_retweet = pd.DataFrame(bag_of_words_no_retweet_df.mean(axis=0), \n",
    "                                     index=bag_of_words_no_retweet_df.columns, columns=['Avg count (no retweet)'])\n",
    "freq_words_some_retweet = pd.DataFrame(bag_of_words_some_retweet_df.mean(axis=0), \n",
    "                                     index=bag_of_words_some_retweet_df.columns, columns=['Avg count (some retweet)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words_no_retweet.join(freq_words_some_retweet, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a sentiment dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download hedonometer's sentiment dictionary from http://hedonometer.org/index.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "url='http://hedonometer.org/api/v1/words/?format=json'\n",
    "data = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "loaded_json = json.loads(data)\n",
    "happ_dict = loaded_json['objects']\n",
    "from pandas.io.json import json_normalize\n",
    "happ_df = json_normalize(happ_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happ_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happ_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_happs_df = happ_df[['word', 'happs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_happs_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dummy variable for when the tweet contains a frequently used word, such as 'smallbiz'. Also, create a count of the number of handles in each tweet. In addition, calculate each tweet's happiness score (a sum of the words' happiness scores). Include other features that you think might be important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)  # Reset the index of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "smallbiz_dummy = []\n",
    "handle_count = []\n",
    "happs_list = []\n",
    "for i in range(0, len(df)):\n",
    "    tweet = df.loc[i]['cleanTweetText']\n",
    "    smallbiz_dummy.append(int('smallbiz' in set(tweet.split())))\n",
    "    handle_count.append(tweet.count('@')) \n",
    "    tweet_df = pd.DataFrame(pd.Series(tweet.split()), columns=['word'])\n",
    "    tweet_happs_df = pd.merge(tweet_df, word_happs_df, on='word')\n",
    "    happs_list.append(tweet_happs_df['happs'].sum())\n",
    "df['contains_smallbiz'] = smallbiz_dummy\n",
    "df['handle_count'] = handle_count\n",
    "df['happ_score'] = happs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a regression tree using the log transformation of RT_Count_in_TimeWindow as the dependent variable. Thus, features will be important on the log scale (or on a percentage basis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_variables_selected = ['Followers_Count', 'contains_smallbiz', 'handle_count', 'happ_score']\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "X_train = df[ind_variables_selected]\n",
    "y_train = np.log(1 + df['RT_Count_in_TimeWindow'])\n",
    "rt = DecisionTreeRegressor(min_samples_split=20, random_state=201)\n",
    "rt_model = rt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features are the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rt_model.feature_importances_, index=ind_variables_selected)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
