{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from wordcloud import WordCloud\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download two of the nltk corpora (see the full list at http://nltk.org/nltk_data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain key codes to access Twitter's API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sign in with your existing Twitter account (or make a new one) at https://twitter.com/.\n",
    "\n",
    "Go to https://developer.twitter.com and register to be a developer. Select personal use when asked. Answer four questions indicating use on a student project. \n",
    "\n",
    "After completing the developer registration process, select \"Create an app\" to make a new application.\n",
    "Fill out the form and feel free to use very generic information (i.e. App name: Your own name, Website URL: \"https://www.darden.virginia.edu/\", Description: Collect and analyze text). You can ignore the everything after Website URL, except for how the app will be used. Tell us how the app will be used: \"This app will collect and analyze text for learning purposes. The app will be written in Python using the tweepy package.\" Click Create.\n",
    "\n",
    "Go to \"Keys and tokens\" tab within the app you create, and create \"Access token & access token secret\". (\"Consumer API keys\" should already be generated.)\n",
    "\n",
    "Go to the Permissions tab and change the access permission to \"Read-only\", because you are not using the code to write tweets back to the site.\n",
    "\n",
    "Save your \"Consumer API keys\" and \"Access token and access token secret\" in a safe place. Put these four codes in place of \"xxx\" in cell below. Be sure to keep the quotation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is commented out becuase we load in the tweets from a file\n",
    "# consumer_key = \"xxx\"\n",
    "# consumer_secret = \"xxx\"\n",
    "# access_token = \"xxx\"\n",
    "# access_token_secret = \"xxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will provide access to Twitter's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "# api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download some recent tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the 1000 latest tweets that use a key word. The cell below runs a for-loop in Python and appends each new tweet to the object called results, which is of class \"list\". In this class, we have the method \"append\", which is a function that appends the item inside the paratheses to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_words = 'star wars'\n",
    "# results = []\n",
    "# for tweet in tweepy.Cursor(api.search, q = key_words, lang = 'en').items(1000): \n",
    "#     results.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the tweets using the pickle libray instead of reading them from the twitter API\n",
    "key_words = 'star wars'\n",
    "import pickle\n",
    "\n",
    "with open ('saved_tweets.pkl', 'rb') as fp:\n",
    "    results = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the first five items in the list. These items in the resulting list are difficult to interpret. They are pieces of a raw json file (a standard file format for storing web-based data). Below we parse our list of tweets so that they are easier to interpret. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]._json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract some information on each of the first five tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in results[:5]:\n",
    "    print(tweet.text, tweet.created_at, tweet.user.time_zone, tweet.user.screen_name, \n",
    "          tweet.user.followers_count, tweet.retweet_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the extracted tweet information into a data frame. First create an empty data frame. Then add columns for some key variables. Each column is a list (anything inside []), which is created with a for-loop. For instance, for each tweet in the big results list, go to its text (via tweet.text) and put it in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['tweetText'] = [tweet.text for tweet in results]\n",
    "df['tweetCreated'] = [tweet.created_at for tweet in results]\n",
    "df['userTimeZone'] = [tweet.user.time_zone for tweet in results]\n",
    "df['userScreenName'] = [tweet.user.screen_name for tweet in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add followers count and retweet count, we need to know if the tweet is a retweet or not. The best way to check (using try/except) is to see if the key 'retweet_status' is a part of the tweet's information. If it is, then we take the followers count of the person who tweeted the original tweet. In this case, we also take the retweet count of the original tweet. Otherwise, we grab the followers and retweet counts of the current tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_count_list = []\n",
    "for tweet in results:\n",
    "    try:\n",
    "        followers_count_list.append(tweet.retweeted_status.user.followers_count)\n",
    "    except AttributeError:\n",
    "        followers_count_list.append(tweet.user.followers_count)\n",
    "df['followersCount'] = followers_count_list\n",
    "\n",
    "retweet_count_list = []\n",
    "for tweet in results:\n",
    "    try:\n",
    "        retweet_count_list.append(tweet.retweeted_status.retweet_count)\n",
    "    except AttributeError:\n",
    "        retweet_count_list.append(tweet.retweet_count)\n",
    "df['retweetCount'] = retweet_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text in the tweets to all lower case. Remove the stopwords (such as 'a', 'the', and 'to') from the tweets. Keep each word's lemma, e.g., the verbs gone, going, and went have the lemma go. The lemmatizer needs to know the word's part of speech (pos). Use the regular expressions package re to remove http, RT, numbers and punctuations (except # and @), and whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = list(set(stopwords.words('english')))\n",
    "print(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "[wordnet_lemmatizer.lemmatize(\"gone\", pos=\"v\"), \n",
    " wordnet_lemmatizer.lemmatize('going', pos=\"v\"), \n",
    " wordnet_lemmatizer.lemmatize('went', pos=\"v\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_text = []\n",
    "for tweet in df['tweetText']:  # Loop through the tokens (the words or symbols) in each tweet.    \n",
    "    cleaned_tweet = re.sub(r\"(RT)\",\" \", tweet)  # Remove RT.\n",
    "    cleaned_tweet = cleaned_tweet.lower()  # Convert the text to lower case\n",
    "    cleaned_tweet = ' '.join([word for word in cleaned_tweet.split() if word not in stopset])  # Keep only words that are not stopwords.\n",
    "    cleaned_tweet = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='n') for word in cleaned_tweet.split()])  # Keep each noun's lemma.\n",
    "    cleaned_tweet = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='v') for word in cleaned_tweet.split()])  # Keep each verb's lemma.\n",
    "    cleaned_tweet = re.sub(r\"amp\",\" \", cleaned_tweet)  # Remove the word 'amp'.\n",
    "    cleaned_tweet = re.sub(r\"(http\\S+)\",\" \", cleaned_tweet)  # Remove http links.\n",
    "    cleaned_tweet = re.sub(\"[^a-zA-Z#@]\",\" \", cleaned_tweet)  # Remove numbers and punctuations except # and @.\n",
    "    cleaned_tweet = ' '.join(cleaned_tweet.split())  # Remove white space.\n",
    "    cleaned_tweet = cleaned_tweet.replace(key_words, \"\")  # Replace your key words.\n",
    "    clean_tweets_text.append(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column for the clean tweets to the existing data frame and print the new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanTweetText'] = clean_tweets_text\n",
    "df[['tweetText', 'cleanTweetText']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some retweets will appear several times so we drop the duplicates. The remaining tweets will be less than 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(['cleanTweetText'], keep='first')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the correlation between followers count and retweet count. Why might we expect them to be correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(df['followersCount'], df['retweetCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['followersCount'], df['retweetCount'])\n",
    "plt.xlabel('Followers Count')\n",
    "plt.ylabel('Retweet Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we apply a log transform to followers count and retweet count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(np.log(1+df['followersCount']), np.log(1+df['retweetCount']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.log(1+df['followersCount']), np.log(1+df['retweetCount']))\n",
    "plt.xlabel('log of Followers Count')\n",
    "plt.ylabel('log of Retweet Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the counts for each of retweet count levels. Are most tweets retweeted or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['retweetCount'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this same information in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['retweetCount'].hist(bins=100, range=(0,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two subsetted data frames -- one for tweets without any retweets and another for tweets that were retweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_retweet = df[df['retweetCount'] == 0]\n",
    "df_some_retweet = df[df['retweetCount'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all the cleaned tweet texts into one long sentence. Then make a wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_long_string =  ' '.join(df['cleanTweetText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace variants of your key words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_long_string = one_long_string.replace('starwars', '')\n",
    "one_long_string = one_long_string.replace('star', '')\n",
    "one_long_string = one_long_string.replace('wars', '')\n",
    "one_long_string = one_long_string.replace('war', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(one_long_string)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Tweets with the key words: ' + key_words)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two more wordclouds -- one for tweets without any retweets and another for tweets that were retweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_long_string_no_retweet =  ' '.join(df_no_retweet['cleanTweetText'])\n",
    "one_long_string_some_retweet =  ' '.join(df_some_retweet['cleanTweetText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_long_string_no_retweet = one_long_string_no_retweet.replace('starwars', '')\n",
    "one_long_string_no_retweet = one_long_string_no_retweet.replace('star', '')\n",
    "one_long_string_no_retweet = one_long_string_no_retweet.replace('wars', '')\n",
    "one_long_string_no_retweet = one_long_string_no_retweet.replace('war', '')\n",
    "one_long_string_some_retweet = one_long_string_some_retweet.replace('starwars', '')\n",
    "one_long_string_some_retweet = one_long_string_some_retweet.replace('star', '')\n",
    "one_long_string_some_retweet = one_long_string_some_retweet.replace('wars', '')\n",
    "one_long_string_some_retweet = one_long_string_some_retweet.replace('war', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any differences in the frequently used words in the non-retweeted and retweeted tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_no_retweet = WordCloud(random_state=201).generate(one_long_string_no_retweet)\n",
    "wordcloud_some_retweet = WordCloud(random_state=201).generate(one_long_string_some_retweet)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(121)  # 121 means 1 row and 2 columns of plots and this is the first subplot.\n",
    "plt.imshow(wordcloud_no_retweet)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Non-retweeted tweets with the key words: ' + key_words)\n",
    "\n",
    "plt.subplot(122)  # 122 means 1 row and 2 columns of plots and this is the second subplot.\n",
    "plt.imshow(wordcloud_some_retweet)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Retweeted tweets with the key words: ' + key_words)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corpus (a list of all your documents). Find all the one-word phrases (unigrams) and two-word phrases (bigrams). We could keep going higher, to find all n-word phrases (ngrams). Below we create a \"bag of words\" for the top 20 phrases (unigrams or bigrams in this case). A bag of words (or document-term matrix) is a data frame of phrase counts. Each row is a document (or tweet in this case). The columns correspond to a phrase in any of the documents. An entry in the data frame is a count of the times the phrase appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(df['cleanTweetText'])\n",
    "corpus_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=30)\n",
    "bag_of_words = corpus_vectorizer.fit_transform(corpus)\n",
    "bag_of_words_df = pd.DataFrame(bag_of_words.toarray(), columns=corpus_vectorizer.get_feature_names())\n",
    "bag_of_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bag_of_words_df.mean(axis=0), index=bag_of_words_df.columns, columns=['Avg count (all tweets)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create two bag of words -- one for tweets with no retweets and another for tweets with some retweets. Are the frequently used words different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_no_retweet = list(df_no_retweet['cleanTweetText'])\n",
    "corpus_no_retweet_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=20)\n",
    "bag_of_words_no_retweet = corpus_no_retweet_vectorizer.fit_transform(corpus_no_retweet)\n",
    "bag_of_words_no_retweet_df = pd.DataFrame(bag_of_words_no_retweet.toarray(), columns=corpus_no_retweet_vectorizer.get_feature_names())\n",
    "bag_of_words_no_retweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_some_retweet = list(df_some_retweet['cleanTweetText'])\n",
    "corpus_some_retweet_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=20)\n",
    "bag_of_words_some_retweet = corpus_some_retweet_vectorizer.fit_transform(corpus_some_retweet)\n",
    "bag_of_words_some_retweet_df = pd.DataFrame(bag_of_words_some_retweet.toarray(), columns=corpus_some_retweet_vectorizer.get_feature_names())\n",
    "bag_of_words_some_retweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words_no_retweet = pd.DataFrame(bag_of_words_no_retweet_df.mean(axis=0), \n",
    "                                     index=bag_of_words_no_retweet_df.columns, columns=['Avg count (no retweet)'])\n",
    "freq_words_some_retweet = pd.DataFrame(bag_of_words_some_retweet_df.mean(axis=0), \n",
    "                                     index=bag_of_words_some_retweet_df.columns, columns=['Avg count (some retweet)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words_no_retweet.join(freq_words_some_retweet, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download hedonometer's sentiment dictionary from http://hedonometer.org/index.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "url='http://hedonometer.org/api/v1/words/?format=json'\n",
    "data = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "loaded_json = json.loads(data)\n",
    "loaded_json    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happ_dict = loaded_json['objects']\n",
    "happ_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "happ_df = json_normalize(happ_dict)\n",
    "happ_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happ_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_happs_df = happ_df[['word', 'happs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_happs_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dummy variable a tweet containing a frequently used word, such as 'force'. Also, create a count of the number of handles in each tweet. In addition, calculate each tweet's happiness score (a sum of the words' happiness scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)  # Reset the index of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_dummy = []\n",
    "handle_count = []\n",
    "happs_list = []\n",
    "for i in range(0, len(df)):\n",
    "    tweet = df.loc[i]['cleanTweetText']\n",
    "    force_dummy.append(int('resistance' in set(tweet.split())))\n",
    "    handle_count.append(tweet.count('@')) \n",
    "    tweet_df = pd.DataFrame(pd.Series(tweet.split()), columns=['word'])  # Create a single column data frame of tweet's words.\n",
    "    tweet_happs_df = pd.merge(tweet_df, word_happs_df, on='word')\n",
    "    happs_list.append(tweet_happs_df['happs'].sum())\n",
    "df['contains_resistance'] = force_dummy\n",
    "df['handle_count'] = handle_count\n",
    "df['happ_score'] = happs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_variables_selected = ['followersCount', 'contains_resistance', 'handle_count', 'happ_score']\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "X_train = df[ind_variables_selected]\n",
    "y_train = df['retweetCount']\n",
    "rt = DecisionTreeRegressor(min_samples_split=2, max_depth=20, random_state=201)\n",
    "rt_model = rt.fit(X_train, y_train)\n",
    "rt_pred = rt_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the variable importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rt_model.feature_importances_, index=ind_variables_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rt_pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your regression tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string buffer dot_data \n",
    "dot_data = StringIO()\n",
    "# This function export the decision tree to the string buffer \"dot_data\" in Graphviz’s Dot format. \n",
    "export_graphviz(rt_model, out_file = dot_data, feature_names = ind_variables_selected, rounded = True,  \n",
    "                proportion = True, rotate = 1, filled = True, node_ids=True)\n",
    "# Create a Python interface to Graphviz’s Dot language.\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "# Save your regression tree. Open the PDF file from the folder location of this code. \n",
    "Image(graph.create_png())\n",
    "graph.write_pdf(\"regressionTree.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
